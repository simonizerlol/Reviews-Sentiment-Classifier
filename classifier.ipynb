{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "COMP551 A3 Sentiment Classification\n",
    "Name: Simon Hsu\n",
    "ID: 260610820\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import sklearn.metrics\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.tree\n",
    "import sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read yelp and imdb csv files\n",
    "yelp_test = pd.read_csv('given_yelp-test.txt', sep = \"\\t\", header = None)\n",
    "yelp_train = pd.read_csv('given_yelp-train.txt', sep = \"\\t\", header = None)\n",
    "yelp_valid = pd.read_csv('given_yelp-valid.txt', sep = \"\\t\", header = None)\n",
    "\n",
    "imdb_test = pd.read_csv('given_IMDB-test.txt', sep = \"\\t\", header = None)\n",
    "imdb_train = pd.read_csv('given_IMDB-train.txt', sep = \"\\t\", header = None)\n",
    "imdb_valid = pd.read_csv('given_IMDB-valid.txt', sep = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question 1: convert the review to a fixed length vector representation(binary bag-of-words and frequency bag-of-words)\n",
    "# convert both the datasets into both these representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick top 10,000 words in the vocabulary and ignore the rest of the words\n",
    "def pickTopWords(reviews):\n",
    "    words = []\n",
    "    topWords = []\n",
    "    \n",
    "    for review in reviews[0]:\n",
    "        # required tasks: punctuation removal and lower-casing the words\n",
    "        words.extend(review.lower().translate(str.maketrans(\"\",\"\", string.punctuation)).split(' '))\n",
    "    \n",
    "    ctr = Counter()\n",
    "    for word in words:\n",
    "        if (word != ''):\n",
    "            ctr[word] += 1\n",
    "        \n",
    "    topWords = ctr.most_common(10000) # pick top 10,000 words\n",
    "    return {tupl[0]: index for index, tupl in enumerate(topWords)}, [tupl[0] + '\\t' + str(index) + '\\t' + str(tupl[1])  for index, tupl in enumerate(topWords)]\n",
    "    \n",
    "# we only need to consider the training set\n",
    "yelp_topWords, yelp_output = pickTopWords(yelp_train)\n",
    "imdb_topWords, imdb_output = pickTopWords(imdb_train)\n",
    "\n",
    "# save the vocabulary of the two datasets into .txt files\n",
    "f = open(\"yelp-vocab.txt\", 'w',encoding='utf-8')\n",
    "ctr = 0\n",
    "for line in yelp_output:\n",
    "    if ctr == 0:  \n",
    "        f.write(line)\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        f.write(line)\n",
    "    ctr += 1\n",
    "f.close()\n",
    "\n",
    "f = open(\"IMDB-vocab.txt\", 'w',encoding='utf-8')\n",
    "ctr = 0\n",
    "for line in imdb_output:\n",
    "    if ctr == 0:  \n",
    "        f.write(line)\n",
    "    else:\n",
    "        f.write('\\n')\n",
    "        f.write(line)\n",
    "    ctr += 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word in the vocabulary has a corresponding numeric id and frequency all tab separated\n",
    "def saveReviewsIDs(dic, reviews, filename):\n",
    "    f = open(filename, 'w',encoding='utf-8')\n",
    "    ctr = 0\n",
    "\n",
    "    for review, categ in zip(reviews[0], reviews[1]):\n",
    "        if ctr != 0:\n",
    "            f.write('\\n')\n",
    "\n",
    "        r = review.lower().translate(str.maketrans(\"\",\"\", string.punctuation)).split(' ')\n",
    "\n",
    "        for index, word in enumerate(r):\n",
    "            if word in dic:\n",
    "                if index != 0:\n",
    "                    f.write(' ')\n",
    "                f.write(str(dic[word]))\n",
    "        f.write('\\t')\n",
    "        f.write(str(categ))\n",
    "        ctr += 1\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saveReviewsIDs(yelp_topWords, yelp_test, \"yelp-test.txt\")\n",
    "saveReviewsIDs(yelp_topWords, yelp_train, \"yelp-train.txt\")\n",
    "saveReviewsIDs(yelp_topWords, yelp_valid, \"yelp-valid.txt\")\n",
    "\n",
    "saveReviewsIDs(imdb_topWords, imdb_test, \"IMDB-test.txt\")\n",
    "saveReviewsIDs(imdb_topWords, imdb_train, \"IMDB-train.txt\")\n",
    "saveReviewsIDs(imdb_topWords, imdb_valid, \"IMDB-valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each of the top 10000 words, there is one corresponding dimension in the feature vector \n",
    "# that is 1 if the example contains the word, and 0 otherwise\n",
    "def bin_bow_vector_gen(topWords, reviews):\n",
    "    m = []\n",
    "    for review in reviews[0]:\n",
    "        vector = [0] * len(topWords)\n",
    "        for word in review:\n",
    "            if word in topWords:\n",
    "                vector[topWords[word]] = 1\n",
    "        m.append(vector)\n",
    "    return np.array(m)\n",
    "\n",
    "# for each of the 10000 words, the corresponding feature is the frequency of occurrence of that word in the given review.\n",
    "# calculate the frequency by summing the occurences of words in a review\n",
    "# and then divide by the sum of occurrences of all 10000 words so that the vector for each example sums to 1\n",
    "def freq_bow_vector_gen(topWords, reviews):\n",
    "    m = []\n",
    "    for review in reviews[0]:\n",
    "        vector = [0] * len(topWords)\n",
    "        for word in review:\n",
    "            if word in topWords:\n",
    "                vector[topWords[word]] += 1\n",
    "                \n",
    "        s = sum(vector)\n",
    "        if s > 0:\n",
    "            vector = np.divide(vector, s)\n",
    "        m.append(vector)\n",
    "    return np.array(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert both the datasets into both these representations\n",
    "yelp_train_bin_bow = bin_bow_vector_gen(yelp_topWords, yelp_train)\n",
    "yelp_test_bin_bow = bin_bow_vector_gen(yelp_topWords, yelp_test)\n",
    "yelp_valid_bin_bow = bin_bow_vector_gen(yelp_topWords, yelp_valid)\n",
    "\n",
    "yelp_train_freq_bow = freq_bow_vector_gen(yelp_topWords, yelp_train)\n",
    "yelp_test_freq_bow = freq_bow_vector_gen(yelp_topWords, yelp_test)\n",
    "yelp_valid_freq_bow = freq_bow_vector_gen(yelp_topWords, yelp_valid)\n",
    "\n",
    "imdb_train_bin_bow = bin_bow_vector_gen(imdb_topWords, imdb_train)\n",
    "imdb_test_bin_bow = bin_bow_vector_gen(imdb_topWords, imdb_test)\n",
    "imdb_valid_bin_bow = bin_bow_vector_gen(imdb_topWords, imdb_valid)\n",
    "\n",
    "imdb_train_freq_bow = freq_bow_vector_gen(imdb_topWords, imdb_train)\n",
    "imdb_test_freq_bow = freq_bow_vector_gen(imdb_topWords, imdb_test)\n",
    "imdb_valid_freq_bow = freq_bow_vector_gen(imdb_topWords, imdb_valid)\n",
    "\n",
    "# end of question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2: yelp dataset with binary bag-of-words representation\n",
    "# use the F1-measure as the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1955"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report the performance of the random classifier\n",
    "random = np.random.choice([1,2,3,4,5], len(yelp_test[1]))\n",
    "sklearn.metrics.f1_score(yelp_test[1], random, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.351"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report the performance of the majority-class classifier\n",
    "majority = np.argmax(np.bincount(yelp_train[1]))\n",
    "majority_array = np.array([majority]*len(yelp_test[1]))\n",
    "sklearn.metrics.f1_score(yelp_test[1], majority_array, average = 'micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3645"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Naive Bayes with Bernoulli Naive Bayes\n",
    "BNB = sklearn.naive_bayes.BernoulliNB()\n",
    "BNB.fit(yelp_train_bin_bow, yelp_train[1])\n",
    "predictions = BNB.predict(yelp_test_bin_bow)\n",
    "sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 2.693877551020408\n",
      "The maximum F-Measure for valid is 0.369\n",
      "The F-Measure for test is 0.36250000000000004\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes hyper-parameter tuning\n",
    "alphas = np.linspace(2, 4, 50)\n",
    "f1s = []\n",
    "for a in alphas:\n",
    "    BNB = sklearn.naive_bayes.BernoulliNB(alpha = a)\n",
    "    BNB.fit(yelp_train_bin_bow, yelp_train[1])\n",
    "    predictions = BNB.predict(yelp_valid_bin_bow)\n",
    "    f1s.append(sklearn.metrics.f1_score(yelp_valid[1], predictions, average = 'micro'))\n",
    "\n",
    "bestAlpha = alphas[np.argmax(f1s)]\n",
    "print(\"The best alpha is \" + str(bestAlpha))\n",
    "print(\"The maximum F-Measure for valid is \" + str(np.max(f1s)))\n",
    "\n",
    "BNB = sklearn.naive_bayes.BernoulliNB(alpha = bestAlpha)\n",
    "BNB.fit(yelp_train_bin_bow, yelp_train[1])\n",
    "predictions = BNB.predict(yelp_test_bin_bow)\n",
    "print(\"The F-Measure for test is \" + str(sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3215"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Decision Trees\n",
    "DT = sklearn.tree.DecisionTreeClassifier()\n",
    "DT.fit(yelp_train_bin_bow, yelp_train[1])\n",
    "predictions = DT.predict(yelp_test_bin_bow)\n",
    "sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F-Measure for valid is 0.313\n",
      "For criteria entropy and splitter random\n",
      "the F-Measure for test is 0.3265\n"
     ]
    }
   ],
   "source": [
    "# Decision Trees hyper-parameter tuning\n",
    "f1s = []\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "splitter = [\"best\", \"random\"]\n",
    "\n",
    "for c in criterion:\n",
    "    for s in splitter:\n",
    "        DT = sklearn.tree.DecisionTreeClassifier(criterion = c, splitter = s)\n",
    "        DT.fit(yelp_train_bin_bow, yelp_train[1])\n",
    "        predictions = DT.predict(yelp_valid_bin_bow)\n",
    "        f1s.append(sklearn.metrics.f1_score(yelp_valid[1], predictions, average = 'micro'))\n",
    "        \n",
    "highestF1 = np.max(f1s)\n",
    "print(\"The highest F-Measure for valid is \"+ str(highestF1))\n",
    "arg = np.argmax(f1s)\n",
    "if arg == 0 or arg == 1:\n",
    "    c = \"gini\"\n",
    "else:\n",
    "    c = \"entropy\"\n",
    "if arg == 0 or arg == 2:\n",
    "    s = \"best\"\n",
    "else:\n",
    "    s = \"random\"\n",
    "    \n",
    "print(\"For criteria \" + str(c) + \" and splitter \" + str(s))\n",
    "\n",
    "DT = sklearn.tree.DecisionTreeClassifier(criterion = c, splitter = s)\n",
    "DT.fit(yelp_train_bin_bow, yelp_train[1])\n",
    "predictions = DT.predict(yelp_test_bin_bow)\n",
    "print(\"the F-Measure for test is \" + str(sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37050000000000005"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Linear SVM\n",
    "svm = sklearn.svm.LinearSVC()\n",
    "svm.fit(yelp_train_bin_bow, yelp_train[1])\n",
    "predictions = svm.predict(yelp_test_bin_bow)\n",
    "sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F-Measure for valid is 0.347\n",
      "For penalty l1 and loss -\n",
      "the F-Measure for test is 0.3685\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM hyper-parameter tuning\n",
    "f1s = []\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "loss = [\"hinge\", \"squared_hinge\"]\n",
    "\n",
    "for p in penalty:\n",
    "    if p == \"l1\": # combination of \"l1\" and (\"hinge\" or \"squared_hinge\") is not supported\n",
    "        svm = sklearn.svm.LinearSVC(penalty = p, dual = False)\n",
    "        svm.fit(yelp_train_bin_bow, yelp_train[1])\n",
    "        predictions = svm.predict(yelp_valid_bin_bow)\n",
    "        f1s.append(sklearn.metrics.f1_score(yelp_valid[1], predictions, average = 'micro'))\n",
    "    else:\n",
    "        for l in loss:\n",
    "            svm = sklearn.svm.LinearSVC(penalty = p, loss = l)\n",
    "            svm.fit(yelp_train_bin_bow, yelp_train[1])\n",
    "            predictions = svm.predict(yelp_valid_bin_bow)\n",
    "            f1s.append(sklearn.metrics.f1_score(yelp_valid[1], predictions, average = 'micro'))\n",
    "        \n",
    "highestF1 = np.max(f1s)\n",
    "print(\"The highest F-Measure for valid is \"+ str(highestF1))\n",
    "arg = np.argmax(f1s)\n",
    "if arg == 0:\n",
    "    p = \"l1\"\n",
    "    l = \"-\"\n",
    "elif arg == 1:\n",
    "    p = \"l2\"\n",
    "    l = \"hinge\"\n",
    "elif arg == 2:\n",
    "    p = \"l2\"\n",
    "    l = \"squared_hinge\"\n",
    "    \n",
    "print(\"For penalty \" + str(p) + \" and loss \" + str(l))\n",
    "\n",
    "if l == \"-\":\n",
    "    svm = sklearn.svm.LinearSVC(penalty = p, dual = False)\n",
    "else:\n",
    "    svm = sklearn.svm.LinearSVC(penalty = p, loss = l)\n",
    "svm.fit(yelp_train_bin_bow, yelp_train[1])\n",
    "predictions = svm.predict(yelp_test_bin_bow)\n",
    "print(\"the F-Measure for test is \" + str(sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')))\n",
    "\n",
    "# end of question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question 3: repeat question 2 but with frequency bag-of-words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2865"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Naive Bayes with Gaussian Naive Bayes\n",
    "GNB = sklearn.naive_bayes.GaussianNB()\n",
    "GNB.fit(yelp_train_freq_bow, yelp_train[1])\n",
    "predictions = GNB.predict(yelp_test_freq_bow)\n",
    "sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Decision Trees\n",
    "DT = sklearn.tree.DecisionTreeClassifier()\n",
    "DT.fit(yelp_train_freq_bow, yelp_train[1])\n",
    "predictions = DT.predict(yelp_test_freq_bow)\n",
    "sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F-Measure for valid is 0.28\n",
      "For criteria entropy and splitter random\n",
      "the F-Measure for test is 0.295\n"
     ]
    }
   ],
   "source": [
    "# Decision Trees hyper-parameter tuning\n",
    "f1s = []\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "splitter = [\"best\", \"random\"]\n",
    "\n",
    "for c in criterion:\n",
    "    for s in splitter:\n",
    "        DT = sklearn.tree.DecisionTreeClassifier(criterion = c, splitter = s)\n",
    "        DT.fit(yelp_train_freq_bow, yelp_train[1])\n",
    "        predictions = DT.predict(yelp_valid_freq_bow)\n",
    "        f1s.append(sklearn.metrics.f1_score(yelp_valid[1], predictions, average = 'micro'))\n",
    "        \n",
    "highestF1 = np.max(f1s)\n",
    "print(\"The highest F-Measure for valid is \"+ str(highestF1))\n",
    "arg = np.argmax(f1s)\n",
    "if arg == 0 or arg == 1:\n",
    "    c = \"gini\"\n",
    "else:\n",
    "    c = \"entropy\"\n",
    "if arg == 0 or arg == 2:\n",
    "    s = \"best\"\n",
    "else:\n",
    "    s = \"random\"\n",
    "    \n",
    "print(\"For criteria \" + str(c) + \" and splitter \" + str(s))\n",
    "\n",
    "DT = sklearn.tree.DecisionTreeClassifier(criterion = c, splitter = s)\n",
    "DT.fit(yelp_train_freq_bow, yelp_train[1])\n",
    "predictions = DT.predict(yelp_test_freq_bow)\n",
    "print(\"the F-Measure for test is \" + str(sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39050000000000007"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Linear SVM\n",
    "svm = sklearn.svm.LinearSVC()\n",
    "svm.fit(yelp_train_freq_bow, yelp_train[1])\n",
    "predictions = svm.predict(yelp_test_freq_bow)\n",
    "sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F-Measure for valid is 0.377\n",
      "For penalty l2 and loss squared_hinge\n",
      "the F-Measure for test is 0.39050000000000007\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM hyper-parameter tuning\n",
    "f1s = []\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "loss = [\"hinge\", \"squared_hinge\"]\n",
    "\n",
    "for p in penalty:\n",
    "    if p == \"l1\": # combination of \"l1\" and (\"hinge\" or \"squared_hinge\") is not supported\n",
    "        svm = sklearn.svm.LinearSVC(penalty = p, dual = False)\n",
    "        svm.fit(yelp_train_freq_bow, yelp_train[1])\n",
    "        predictions = svm.predict(yelp_valid_freq_bow)\n",
    "        f1s.append(sklearn.metrics.f1_score(yelp_valid[1], predictions, average = 'micro'))\n",
    "    else:\n",
    "        for l in loss:\n",
    "            svm = sklearn.svm.LinearSVC(penalty = p, loss = l)\n",
    "            svm.fit(yelp_train_freq_bow, yelp_train[1])\n",
    "            predictions = svm.predict(yelp_valid_freq_bow)\n",
    "            f1s.append(sklearn.metrics.f1_score(yelp_valid[1], predictions, average = 'micro'))\n",
    "        \n",
    "highestF1 = np.max(f1s)\n",
    "print(\"The highest F-Measure for valid is \"+ str(highestF1))\n",
    "arg = np.argmax(f1s)\n",
    "if arg == 0:\n",
    "    p = \"l1\"\n",
    "    l = \"-\"\n",
    "elif arg == 1:\n",
    "    p = \"l2\"\n",
    "    l = \"hinge\"\n",
    "elif arg == 2:\n",
    "    p = \"l2\"\n",
    "    l = \"squared_hinge\"\n",
    "    \n",
    "print(\"For penalty \" + str(p) + \" and loss \" + str(l))\n",
    "\n",
    "if l == \"-\":\n",
    "    svm = sklearn.svm.LinearSVC(penalty = p, dual = False)\n",
    "else:\n",
    "    svm = sklearn.svm.LinearSVC(penalty = p, loss = l)\n",
    "svm.fit(yelp_train_freq_bow, yelp_train[1])\n",
    "predictions = svm.predict(yelp_test_freq_bow)\n",
    "print(\"the F-Measure for test is \" + str(sklearn.metrics.f1_score(yelp_test[1], predictions, average = 'micro')))\n",
    "\n",
    "# end of question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question 4.1: repeat question 2 but with IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50272"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report the performance of the random classifier\n",
    "rand = np.random.choice([0,1], len(imdb_test[1]))\n",
    "sklearn.metrics.f1_score(imdb_test[1], rand, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report the performance of the majority-class classifier\n",
    "majority = np.argmax(np.bincount(imdb_train[1]))\n",
    "majority_array = np.array([majority]*len(imdb_test[1]))\n",
    "sklearn.metrics.f1_score(imdb_test[1], majority_array, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55116"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Naive Bayes with Bernoulli Naive Bayes\n",
    "BNB = sklearn.naive_bayes.BernoulliNB()\n",
    "BNB.fit(imdb_train_bin_bow, imdb_train[1])\n",
    "predictions = BNB.predict(imdb_test_bin_bow)\n",
    "sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 3.979591836734694\n",
      "The maximum F-Measure for valid is 0.543\n",
      "The F-Measure for test is 0.5512\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes hyper-parameter tuning\n",
    "alphas = np.linspace(3, 5, 50)\n",
    "f1s = []\n",
    "for a in alphas:\n",
    "    BNB = sklearn.naive_bayes.BernoulliNB(alpha = a)\n",
    "    BNB.fit(imdb_train_bin_bow, imdb_train[1])\n",
    "    predictions = BNB.predict(imdb_valid_bin_bow)\n",
    "    f1s.append(sklearn.metrics.f1_score(imdb_valid[1], predictions, average = 'micro'))\n",
    "\n",
    "bestAlpha = alphas[np.argmax(f1s)]\n",
    "print(\"The best alpha is \" + str(bestAlpha))\n",
    "print(\"The maximum F-Measure for valid is \" + str(np.max(f1s)))\n",
    "\n",
    "BNB = sklearn.naive_bayes.BernoulliNB(alpha = bestAlpha)\n",
    "BNB.fit(imdb_train_bin_bow, imdb_train[1])\n",
    "predictions = BNB.predict(imdb_test_bin_bow)\n",
    "print(\"The F-Measure for test is \" + str(sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53972"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Decision Trees\n",
    "DT = sklearn.tree.DecisionTreeClassifier()\n",
    "DT.fit(imdb_train_bin_bow, imdb_train[1])\n",
    "predictions = DT.predict(imdb_test_bin_bow)\n",
    "sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F-Measure for valid is 0.537\n",
      "For criteria entropy and splitter random\n",
      "the F-Measure for test is 0.53988\n"
     ]
    }
   ],
   "source": [
    "# Decision Trees hyper-parameter tuning\n",
    "f1s = []\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "splitter = [\"best\", \"random\"]\n",
    "\n",
    "for c in criterion:\n",
    "    for s in splitter:\n",
    "        DT = sklearn.tree.DecisionTreeClassifier(criterion = c, splitter = s)\n",
    "        DT.fit(imdb_train_bin_bow, imdb_train[1])\n",
    "        predictions = DT.predict(imdb_valid_bin_bow)\n",
    "        f1s.append(sklearn.metrics.f1_score(imdb_valid[1], predictions, average = 'micro'))\n",
    "        \n",
    "highestF1 = np.max(f1s)\n",
    "print(\"The highest F-Measure for valid is \"+ str(highestF1))\n",
    "arg = np.argmax(f1s)\n",
    "if arg == 0 or arg == 1:\n",
    "    c = \"gini\"\n",
    "else:\n",
    "    c = \"entropy\"\n",
    "if arg == 0 or arg == 2:\n",
    "    s = \"best\"\n",
    "else:\n",
    "    s = \"random\"\n",
    "    \n",
    "print(\"For criteria \" + str(c) + \" and splitter \" + str(s))\n",
    "\n",
    "DT = sklearn.tree.DecisionTreeClassifier(criterion = c, splitter = s)\n",
    "DT.fit(imdb_train_bin_bow, imdb_train[1])\n",
    "predictions = DT.predict(imdb_test_bin_bow)\n",
    "print(\"the F-Measure for test is \" + str(sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55664"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Linear SVM\n",
    "svm = sklearn.svm.LinearSVC()\n",
    "svm.fit(imdb_train_bin_bow, imdb_train[1])\n",
    "predictions = svm.predict(imdb_test_bin_bow)\n",
    "sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F-Measure for valid is 0.5508\n",
      "For penalty l2 and loss hinge\n",
      "the F-Measure for test is 0.55628\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM hyper-parameter tuning\n",
    "f1s = []\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "loss = [\"hinge\", \"squared_hinge\"]\n",
    "\n",
    "for p in penalty:\n",
    "    if p == \"l1\": # combination of \"l1\" and (\"hinge\" or \"squared_hinge\") is not supported\n",
    "        svm = sklearn.svm.LinearSVC(penalty = p, dual = False)\n",
    "        svm.fit(imdb_train_bin_bow, imdb_train[1])\n",
    "        predictions = svm.predict(imdb_valid_bin_bow)\n",
    "        f1s.append(sklearn.metrics.f1_score(imdb_valid[1], predictions, average = 'micro'))\n",
    "    else:\n",
    "        for l in loss:\n",
    "            svm = sklearn.svm.LinearSVC(penalty = p, loss = l)\n",
    "            svm.fit(imdb_train_bin_bow, imdb_train[1])\n",
    "            predictions = svm.predict(imdb_valid_bin_bow)\n",
    "            f1s.append(sklearn.metrics.f1_score(imdb_valid[1], predictions, average = 'micro'))\n",
    "        \n",
    "highestF1 = np.max(f1s)\n",
    "print(\"The highest F-Measure for valid is \"+ str(highestF1))\n",
    "arg = np.argmax(f1s)\n",
    "if arg == 0:\n",
    "    p = \"l1\"\n",
    "    l = \"-\"\n",
    "elif arg == 1:\n",
    "    p = \"l2\"\n",
    "    l = \"hinge\"\n",
    "elif arg == 2:\n",
    "    p = \"l2\"\n",
    "    l = \"squared_hinge\"\n",
    "    \n",
    "print(\"For penalty \" + str(p) + \" and loss \" + str(l))\n",
    "\n",
    "if l == \"-\":\n",
    "    svm = sklearn.svm.LinearSVC(penalty = p, dual = False)\n",
    "else:\n",
    "    svm = sklearn.svm.LinearSVC(penalty = p, loss = l)\n",
    "svm.fit(imdb_train_bin_bow, imdb_train[1])\n",
    "predictions = svm.predict(imdb_test_bin_bow)\n",
    "print(\"the F-Measure for test is \" + str(sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')))\n",
    "\n",
    "# end of question 4.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question 4.2: repeat question 3 but with IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51632"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Naive Bayes with Gaussian Naive Bayes\n",
    "GNB = sklearn.naive_bayes.GaussianNB()\n",
    "GNB.fit(imdb_train_freq_bow, imdb_train[1])\n",
    "predictions = GNB.predict(imdb_test_freq_bow)\n",
    "sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.536"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Decision Trees\n",
    "DT = sklearn.tree.DecisionTreeClassifier()\n",
    "DT.fit(imdb_train_freq_bow, imdb_train[1])\n",
    "predictions = DT.predict(imdb_test_freq_bow)\n",
    "sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F-Measure for valid is 0.5493\n",
      "For criteria entropy and splitter best\n",
      "the F-Measure for test is 0.5408\n"
     ]
    }
   ],
   "source": [
    "# Decision Trees hyper-parameter tuning\n",
    "f1s = []\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "splitter = [\"best\", \"random\"]\n",
    "\n",
    "for c in criterion:\n",
    "    for s in splitter:\n",
    "        DT = sklearn.tree.DecisionTreeClassifier(criterion = c, splitter = s)\n",
    "        DT.fit(imdb_train_freq_bow, imdb_train[1])\n",
    "        predictions = DT.predict(imdb_valid_freq_bow)\n",
    "        f1s.append(sklearn.metrics.f1_score(imdb_valid[1], predictions, average = 'micro'))\n",
    "        \n",
    "highestF1 = np.max(f1s)\n",
    "print(\"The highest F-Measure for valid is \"+ str(highestF1))\n",
    "arg = np.argmax(f1s)\n",
    "if arg == 0 or arg == 1:\n",
    "    c = \"gini\"\n",
    "else:\n",
    "    c = \"entropy\"\n",
    "if arg == 0 or arg == 2:\n",
    "    s = \"best\"\n",
    "else:\n",
    "    s = \"random\"\n",
    "    \n",
    "print(\"For criteria \" + str(c) + \" and splitter \" + str(s))\n",
    "\n",
    "DT = sklearn.tree.DecisionTreeClassifier(criterion = c, splitter = s)\n",
    "DT.fit(imdb_train_freq_bow, imdb_train[1])\n",
    "predictions = DT.predict(imdb_test_freq_bow)\n",
    "print(\"the F-Measure for test is \" + str(sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61228"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Linear SVM\n",
    "svm = sklearn.svm.LinearSVC()\n",
    "svm.fit(imdb_train_freq_bow, imdb_train[1])\n",
    "predictions = svm.predict(imdb_test_freq_bow)\n",
    "sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F-Measure for valid is 0.6205\n",
      "For penalty l1 and loss -\n",
      "the F-Measure for test is 0.61676\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM hyper-parameter tuning\n",
    "f1s = []\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "loss = [\"hinge\", \"squared_hinge\"]\n",
    "\n",
    "for p in penalty:\n",
    "    if p == \"l1\": # combination of \"l1\" and (\"hinge\" or \"squared_hinge\") is not supported\n",
    "        svm = sklearn.svm.LinearSVC(penalty=p, dual=False)\n",
    "        svm.fit(imdb_train_freq_bow, imdb_train[1])\n",
    "        predictions = svm.predict(imdb_valid_freq_bow)\n",
    "        f1s.append(sklearn.metrics.f1_score(imdb_valid[1], predictions, average = 'micro'))\n",
    "    else:\n",
    "        for l in loss:\n",
    "            svm = sklearn.svm.LinearSVC(penalty=p, loss=l)\n",
    "            svm.fit(imdb_train_freq_bow, imdb_train[1])\n",
    "            predictions = svm.predict(imdb_valid_freq_bow)\n",
    "            f1s.append(sklearn.metrics.f1_score(imdb_valid[1], predictions, average = 'micro'))\n",
    "        \n",
    "highestF1 = np.max(f1s)\n",
    "print(\"The highest F-Measure for valid is \"+ str(highestF1))\n",
    "arg = np.argmax(f1s)\n",
    "if arg == 0:\n",
    "    p = \"l1\"\n",
    "    l = \"-\"\n",
    "elif arg == 1:\n",
    "    p = \"l2\"\n",
    "    l = \"hinge\"\n",
    "elif arg == 2:\n",
    "    p = \"l2\"\n",
    "    l = \"squared_hinge\"\n",
    "    \n",
    "print(\"For penalty \" + str(p) + \" and loss \" + str(l))\n",
    "\n",
    "if l == \"-\":\n",
    "    svm = sklearn.svm.LinearSVC(penalty = p, dual = False)\n",
    "else:\n",
    "    svm = sklearn.svm.LinearSVC(penalty = p, loss = l)\n",
    "svm.fit(imdb_train_freq_bow, imdb_train[1])\n",
    "predictions = svm.predict(imdb_test_freq_bow)\n",
    "print(\"the F-Measure for test is \" + str(sklearn.metrics.f1_score(imdb_test[1], predictions, average = 'micro')))\n",
    "\n",
    "# end of question 4.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
